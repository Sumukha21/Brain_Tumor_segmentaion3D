{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oXxwLBH9HD3D",
        "outputId": "bbeda5dc-bca5-413c-d405-02340e51184d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r \"/content/drive/MyDrive/Personal/MS/Brain_Tumor_segmentaion3D/environment.txt\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sYpzJ3_uUxy5",
        "outputId": "6ea32bb2-99cd-457b-bc42-2de64a13c0a1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Description:\n",
        " \n",
        "### Image types: \n",
        "* Native T1-weighted (T1): This scan is obtained using a standard T1-weighted imaging sequence, which uses a short TR (repetition time) and a short TE (echo time) to provide high-resolution images of the brain tissue. This sequence highlights the differences in tissue types based on their contrast with the surrounding tissues.\n",
        "\n",
        "* Post-contrast T1-weighted (T1Gd): This scan is obtained using a T1-weighted imaging sequence after the administration of a contrast agent such as Gadolinium. The contrast agent is injected intravenously and is taken up by cells with a disrupted blood-brain barrier, which is a common characteristic of brain tumors. This sequence highlights the regions of the brain with a disrupted blood-brain barrier, such as enhancing tumor regions.\n",
        "\n",
        "* T2-weighted (T2): This scan is obtained using a T2-weighted imaging sequence, which uses a long TR and a long TE to provide a more detailed view of the brain tissue. This sequence highlights subtle differences in tissue types that are not visible on T1 scans.\n",
        "\n",
        "* T2 Fluid Attenuated Inversion Recovery (T2-FLAIR): This scan is obtained using a T2-weighted imaging sequence that is modified to suppress the signal from cerebrospinal fluid (CSF). This is achieved by using an inversion recovery pulse before the T2-weighted acquisition. This sequence is useful for distinguishing between edema and other types of brain tissue because the CSF signal is suppressed.\n",
        "\n",
        "### Segmentation Classes:\n",
        "* label 0: No tumor\n",
        "* label 1: necrotic tumor core (Visible in T2): This class represents the core of the tumor, which is composed of necrotic tissue and non-enhancing tumor cells.\n",
        "* label 2: the peritumoral edematous/invaded tissue (Visible in flair):  This class represents the edema, or swelling, that occurs around the tumor due to the accumulation of fluid in the surrounding brain tissue.\n",
        "* label 4: Gd-enhancing tumor (Needs to be converted to 3) (Visible in T1ce): This class represents the region of the tumor that enhances with the administration of contrast agent."
      ],
      "metadata": {
        "id": "D3FdRqKZEs45"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tarfile"
      ],
      "metadata": {
        "id": "rFkRj269Hdss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_tarfile = tarfile.open('/kaggle/input/brats-2021-task1/BraTS2021_Training_Data.tar')\n",
        "index = my_tarfile.getnames()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-16T20:55:13.884662Z",
          "iopub.execute_input": "2023-03-16T20:55:13.885074Z",
          "iopub.status.idle": "2023-03-16T20:56:19.341448Z",
          "shell.execute_reply.started": "2023-03-16T20:55:13.885036Z",
          "shell.execute_reply": "2023-03-16T20:56:19.339939Z"
        },
        "trusted": true,
        "id": "GdokNB6LEs48"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index[: 8]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-16T20:58:41.360157Z",
          "iopub.execute_input": "2023-03-16T20:58:41.360558Z",
          "iopub.status.idle": "2023-03-16T20:58:41.367927Z",
          "shell.execute_reply.started": "2023-03-16T20:58:41.360521Z",
          "shell.execute_reply": "2023-03-16T20:58:41.366725Z"
        },
        "trusted": true,
        "id": "FdczVee2Es48",
        "outputId": "70a95eae-264a-42d1-d263-761457a4c9d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 5,
          "output_type": "execute_result",
          "data": {
            "text/plain": "['.',\n './.DS_Store',\n './BraTS2021_00000',\n './BraTS2021_00000/BraTS2021_00000_flair.nii.gz',\n './BraTS2021_00000/BraTS2021_00000_seg.nii.gz',\n './BraTS2021_00000/BraTS2021_00000_t1.nii.gz',\n './BraTS2021_00000/BraTS2021_00000_t1ce.nii.gz',\n './BraTS2021_00000/BraTS2021_00000_t2.nii.gz']"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utilities"
      ],
      "metadata": {
        "id": "Nac3cIwdJREi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import yaml\n",
        "import importlib\n",
        "\n",
        "\n",
        "def check_path(path):\n",
        "    if not os.path.exists(path):\n",
        "        os.mkdir(path)\n",
        "\n",
        "\n",
        "def check_path_recursively(save_folder, config,\n",
        "                           params=[\"model_name\", \"pretrained_name\", \"dataset\", \"similarity_measure\", \"prompt\"]):\n",
        "    params = [config[i] for i in params if config.get(i) is not None]\n",
        "    for i in params:\n",
        "        save_folder = save_folder + \"/\" + i\n",
        "        check_path(save_folder)\n",
        "    config[\"save_folder\"] = save_folder\n",
        "\n",
        "\n",
        "def yaml_writer(path, contents):\n",
        "    with open(path, \"w\") as f:\n",
        "        yaml.dump(contents, f)\n",
        "\n",
        "\n",
        "def text_file_reader(path):\n",
        "    with open(path, \"r\") as f:\n",
        "        contents = list(f.readlines())\n",
        "    contents = [i.split(\"\\n\")[0] for i in contents]\n",
        "    return contents\n",
        "\n",
        "\n",
        "def text_file_writer(path, contents):\n",
        "    with open(path, \"w\") as f:\n",
        "        for line in contents:\n",
        "            if isinstance(line, list):\n",
        "                f.write(\" \".join(line))\n",
        "                f.write(\"\\n\")\n",
        "            else:\n",
        "                f.write(line + \"\\n\")\n",
        "\n",
        "\n",
        "def instantiate_attribute(path):\n",
        "    module_path, attribute_name = path.rsplit(\".\", 1)\n",
        "    module = importlib.import_module(module_path)\n",
        "    return getattr(module, attribute_name)\n",
        "\n",
        "\n",
        "def instantiate_class(path, params):\n",
        "    optimizer_attribute = instantiate_attribute(path)\n",
        "    return optimizer_attribute(params)\n"
      ],
      "metadata": {
        "id": "8NiIFjq7JUHd"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basic Visualization"
      ],
      "metadata": {
        "id": "QjNnUhM-Es49"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install itkwidgets"
      ],
      "metadata": {
        "id": "_zW8wyhfEs49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import nibabel as nib\n",
        "from ipywidgets import interact\n",
        "from tqdm import tqdm\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from scipy.ndimage.measurements import label, center_of_mass"
      ],
      "metadata": {
        "id": "Fq_52J7gExin"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VisualizePatientData:\n",
        "    def __init__(self, patient_data_folder, img_type_id):\n",
        "        self.patient_data_list = sorted(glob.glob(os.path.join(patient_data_folder, \"*\")))\n",
        "        self.image_types = [\"flair\", \"seg\", \"t1\", \"t1ce\", \"t2\"]\n",
        "        self.cmap_list = [\"gray\", \"BuPu\", \"gray\", \"gray\", \"gray\"]\n",
        "        self.i = img_type_id\n",
        "        self.fig = plt.figure(figsize=(1, 1));\n",
        "    \n",
        "    def visualize_brain_scans(self, cube_path):\n",
        "        def create_display(layer):\n",
        "            self.fig.add_subplot(3, 2, self.i + 1)\n",
        "            plt.imshow(self.scans[:, :, layer], cmap=self.cmap_list[self.i]);\n",
        "            plt.axis('off')\n",
        "            return layer\n",
        "        self.scans = np.asarray(nib.load(cube_path).get_fdata())\n",
        "        print(seld.scans.shape)\n",
        "        interact(create_display, layer=(0, self.scans.shape[2] - 1));\n",
        "\n",
        "    def __call__(self, idx):        \n",
        "        data_path = os.path.join(self.patient_data_list[idx], \"BraTS20_Training_%03d_%s.nii\" % (idx + 1, self.image_types[self.i]))\n",
        "        self.visualize_brain_scans(data_path)"
      ],
      "metadata": {
        "id": "sK9W18YYEs49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "patient_data_folder = \"/kaggle/input/brain-tumor-segmentation-in-mri-brats-2015/MICCAI_BraTS2020_TrainingData\"\n",
        "example_patient_id = 0"
      ],
      "metadata": {
        "id": "lvR32GKREs4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualizer_flair = VisualizePatientData(patient_data_folder, 0)\n",
        "visualizer_flair(example_patient_id)"
      ],
      "metadata": {
        "id": "ATSJ7EbtEs4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualizer_seg = VisualizePatientData(patient_data_folder, 1)\n",
        "visualizer_seg(example_patient_id)"
      ],
      "metadata": {
        "id": "Z3aGWLI4Es4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualizer_t1 = VisualizePatientData(patient_data_folder, 2)\n",
        "visualizer_t1(example_patient_id)"
      ],
      "metadata": {
        "id": "MGfSgTAkEs4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualizer_t1ce = VisualizePatientData(patient_data_folder, 3)\n",
        "visualizer_t1ce(example_patient_id)"
      ],
      "metadata": {
        "id": "1fHZRUiOEs4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualizer_t2 = VisualizePatientData(patient_data_folder, 4)\n",
        "visualizer_t2(example_patient_id)"
      ],
      "metadata": {
        "id": "nLurHo_vEs4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Segmentation Classes study"
      ],
      "metadata": {
        "id": "V56L5n-vEs4-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nibabel as nib\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import glob\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from scipy.ndimage.measurements import label, center_of_mass"
      ],
      "metadata": {
        "id": "Z6h3yANvEs4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "patient_data_folder = \"/kaggle/input/brain-tumor-segmentation-in-mri-brats-2015/MICCAI_BraTS2020_TrainingData\"\n",
        "patient_data_list = sorted(glob.glob(os.path.join(patient_data_folder, \"*\")))"
      ],
      "metadata": {
        "id": "rNJQJvZdEs4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "core_tumor = []\n",
        "peritumoral_tissue = []\n",
        "enhancing_tumor = []\n",
        "cube_size = 240 * 240 * 155\n",
        "for i in tqdm(range(len(patient_data_list) - 2)):\n",
        "    if i == 354:\n",
        "        patient_label_data_path = \"/kaggle/input/brain-tumor-segmentation-in-mri-brats-2015/MICCAI_BraTS2020_TrainingData/BraTS20_Training_355/W39_1998.09.19_Segm.nii\"\n",
        "    else:\n",
        "        patient_label_data_path = os.path.join(patient_data_list[i], \"BraTS20_Training_%03d_seg.nii\" % (i + 1))\n",
        "    patient_label_data = nib.load(patient_label_data_path).get_fdata()\n",
        "    core_tumor.append((len(np.where(patient_label_data == 1)[0]) / cube_size) * 100)\n",
        "    peritumoral_tissue.append((len(np.where(patient_label_data == 2)[0]) / cube_size) * 100)\n",
        "    enhancing_tumor.append((len(np.where(patient_label_data == 4)[0]) / cube_size) * 100)"
      ],
      "metadata": {
        "id": "VpkLXrD9Es4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = [core_tumor, peritumoral_tissue, enhancing_tumor]\n",
        "data_string = [\"core_tumor\", \"peritumoral_tissue\", \"enhancing_tumor\"] \n",
        "fig = plt.figure(figsize=(12, 12))\n",
        "for i in range(4):\n",
        "    plt.subplot(2, 2, i + 1)\n",
        "    if i == 3:\n",
        "        plt.bar(x=[0, 1, 2], height=[np.average(core_tumor), np.average(peritumoral_tissue), np.average(enhancing_tumor)])\n",
        "        plt.title(\"Average volume of %s, %s, %s\" % (data_string[0], data_string[1], data_string[2]))\n",
        "    else:    \n",
        "        plt.hist(data[i])\n",
        "        plt.title(\"Distribution of volume of %s\" % data_string[i])"
      ],
      "metadata": {
        "id": "iYreI_TvEs4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_brain_scans(scans):\n",
        "    def create_display(layer):\n",
        "        plt.imshow(scans[:, :, layer], cmap=\"BuPu\");\n",
        "        plt.axis('off')\n",
        "        return layer\n",
        "    interact(create_display, layer=(0, scans.shape[2] - 1));"
      ],
      "metadata": {
        "id": "--hAKugEEs4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_centroid_volume_largest_component(seg_labels, label_id):\n",
        "    seg_labels_core = np.asarray(seg_labels == label_id, dtype=np.uint8)\n",
        "    labels, num_labels = label(seg_labels_core)\n",
        "    volumes = []\n",
        "    for i in range(1, num_labels + 1):\n",
        "        volume = np.sum(labels == i)\n",
        "        volumes.append(volume)\n",
        "    volumes = np.array(volumes)\n",
        "    if len(volumes):\n",
        "        largest_components_id = np.argmax(volumes, -1) + 1\n",
        "        centroid_i = center_of_mass(seg_labels_core, labels=labels, index=largest_components_id)\n",
        "        volume_i = volumes[largest_components_id - 1]\n",
        "        return centroid_i, volume_i\n",
        "    else:\n",
        "        return None, None"
      ],
      "metadata": {
        "id": "UagISeSvEs4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def centroid_volume_correlation(label_id):\n",
        "    centroids = []\n",
        "    volumes = []\n",
        "    for i in tqdm(range(len(patient_data_list) - 2)):\n",
        "        if i == 354:\n",
        "            patient_label_data_path = \"/kaggle/input/brain-tumor-segmentation-in-mri-brats-2015/MICCAI_BraTS2020_TrainingData/BraTS20_Training_355/W39_1998.09.19_Segm.nii\"\n",
        "        else:\n",
        "            patient_label_data_path = os.path.join(patient_data_list[i], \"BraTS20_Training_%03d_seg.nii\" % (i + 1))\n",
        "        patient_label_data = nib.load(patient_label_data_path).get_fdata()\n",
        "        centroid_i, volume_i = compute_centroid_volume_largest_component(patient_label_data, label_id)\n",
        "        if volume_i is None:\n",
        "            continue\n",
        "        centroids.append(centroid_i)\n",
        "        volumes.append(volume_i)\n",
        "    return centroids, volumes"
      ],
      "metadata": {
        "id": "WncW--QgEs4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_id = 1\n",
        "centroids, volumes = centroid_volume_correlation(label_id)\n",
        "centroids = np.array(centroids)\n",
        "volumes = np.array(volumes)"
      ],
      "metadata": {
        "id": "kMjL6RrQEs4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(120, 120))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.scatter(centroids[:,0], centroids[:,1], centroids[:,2], s=volumes, c=volumes, cmap='BuPu')\n",
        "ax.set_xlabel('X Centroid', fontsize=100)\n",
        "ax.set_ylabel('Y Centroid', fontsize=100)\n",
        "ax.set_zlabel('Z Centroid', fontsize=100)\n",
        "ax.set_title('Correlation between Centroid and Volume', fontsize=100)\n",
        "# cbar = plt.colorbar()\n",
        "# cbar.set_label('Volume')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "l8il6MhWEs4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_id = 2\n",
        "centroids, volumes = centroid_volume_correlation(label_id)\n",
        "centroids = np.array(centroids)\n",
        "volumes = np.array(volumes)"
      ],
      "metadata": {
        "id": "BLpq_DGkEs4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(120, 120))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.scatter(centroids[:,0], centroids[:,1], centroids[:,2], s=volumes, c=volumes, cmap='BuPu')\n",
        "ax.set_xlabel('X Centroid', fontsize=100)\n",
        "ax.set_ylabel('Y Centroid', fontsize=100)\n",
        "ax.set_zlabel('Z Centroid', fontsize=100)\n",
        "ax.set_title('Correlation between Centroid and Volume', fontsize=100)\n",
        "# cbar = plt.colorbar()\n",
        "# cbar.set_label('Volume')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "exfQut0YEs4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_id = 4\n",
        "centroids, volumes = centroid_volume_correlation(label_id)\n",
        "centroids = np.array(centroids)\n",
        "volumes = np.array(volumes)\n",
        "fig = plt.figure(figsize=(120, 120))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.scatter(centroids[:,0], centroids[:,1], centroids[:,2], s=volumes, c=volumes, cmap='BuPu')\n",
        "ax.set_xlabel('X Centroid', fontsize=100)\n",
        "ax.set_ylabel('Y Centroid', fontsize=100)\n",
        "ax.set_zlabel('Z Centroid', fontsize=100)\n",
        "ax.set_title('Correlation between Centroid and Volume', fontsize=100)\n",
        "# cbar = plt.colorbar()\n",
        "# cbar.set_label('Volume')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "g0mJmaQwEs4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generating two splits from a dataset"
      ],
      "metadata": {
        "id": "Q0MPK_XPEs5A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import glob\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import nibabel as nib\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "jrTjDmGcEs5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_class_volumes(patient_folder_list):\n",
        "    core_tumor = []\n",
        "    peritumoral_tissue = []\n",
        "    enhancing_tumor = []\n",
        "    cube_size = 240 * 240 * 155\n",
        "    for i in tqdm(patient_folder_list):\n",
        "        if i.endswith(\"355\"):\n",
        "            patient_label_data_path = \"/kaggle/input/brain-tumor-segmentation-in-mri-brats-2015/MICCAI_BraTS2020_TrainingData/BraTS20_Training_355/W39_1998.09.19_Segm.nii\"\n",
        "        else:\n",
        "            patient_label_data_path = os.path.join(i, \"BraTS20_Training_%03d_seg.nii\" % int(i.split(\"_\")[-1]))\n",
        "        patient_label_data = nib.load(patient_label_data_path).get_fdata()\n",
        "        core_tumor.append((len(np.where(patient_label_data == 1)[0]) / cube_size) * 100)\n",
        "        peritumoral_tissue.append((len(np.where(patient_label_data == 2)[0]) / cube_size) * 100)\n",
        "        enhancing_tumor.append((len(np.where(patient_label_data == 4)[0]) / cube_size) * 100)\n",
        "    return core_tumor, peritumoral_tissue, enhancing_tumor"
      ],
      "metadata": {
        "id": "CTut-s-oEs5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_class_distributions(data):\n",
        "    core_tumor, peritumoral_tissue, enhancing_tumor = data\n",
        "    data_string = [\"core_tumor\", \"peritumoral_tissue\", \"enhancing_tumor\"] \n",
        "    fig = plt.figure(figsize=(12, 12))\n",
        "    for i in range(4):\n",
        "        plt.subplot(2, 2, i + 1)\n",
        "        if i == 3:\n",
        "            plt.bar(x=[0, 1, 2], height=[np.average(core_tumor), np.average(peritumoral_tissue), np.average(enhancing_tumor)])\n",
        "            plt.title(\"Average volume of %s, %s, %s\" % (data_string[0], data_string[1], data_string[2]))\n",
        "        else:    \n",
        "            plt.hist(data[i])\n",
        "            plt.title(\"Distribution of volume of %s\" % data_string[i])\n",
        "    return np.average(core_tumor), np.average(peritumoral_tissue), np.average(enhancing_tumor)"
      ],
      "metadata": {
        "id": "lsiF8DF8Es5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_two_splits(data_list, split_ratio):\n",
        "    continue_loop = True\n",
        "    while continue_loop:\n",
        "        list1 = random.sample(data_list, int(0.8 * len(data_list)))\n",
        "        list2 = [i for i in data_list if i not in list1]\n",
        "        list1_volumes = compute_class_volumes(list1)\n",
        "        list2_volumes = compute_class_volumes(list2)\n",
        "        list1_volumes_averages = [np.average(i) for i in list1_volumes]\n",
        "        list2_volumes_averages = [np.average(i) for i in list2_volumes]\n",
        "        overall_averages = [np.average(i + j) for i, j in zip(list1_volumes_averages, list2_volumes_averages)]\n",
        "        list1_class_proportion = np.divide(list1_volumes_averages, np.sum(list1_volumes_averages))\n",
        "        list2_class_proportion = np.divide(list2_volumes_averages, np.sum(list2_volumes_averages))\n",
        "        overall_class_proportion = np.divide(overall_averages, np.sum(overall_averages))\n",
        "        print(\"Current split data: \", list1_class_proportion, list2_class_proportion)\n",
        "        if ((overall_class_proportion[0] - 0.01 <= list1_class_proportion[0] <= overall_class_proportion[0] + 0.01) and\n",
        "           (overall_class_proportion[1] - 0.01 <= list1_class_proportion[1] <= overall_class_proportion[1] + 0.01) and\n",
        "           (overall_class_proportion[2] - 0.01 <= list1_class_proportion[2] <= overall_class_proportion[2] + 0.01) and\n",
        "           (overall_class_proportion[0] - 0.01 <= list2_class_proportion[0] <= overall_class_proportion[0] + 0.01) and\n",
        "           (overall_class_proportion[1] - 0.01 <= list2_class_proportion[1] <= overall_class_proportion[1] + 0.01) and\n",
        "           (overall_class_proportion[2] - 0.01 <= list2_class_proportion[2] <= overall_class_proportion[2] + 0.01)):\n",
        "                return list1, list2"
      ],
      "metadata": {
        "id": "_vdFbPgkEs5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "patient_data_folder = \"/kaggle/input/brain-tumor-segmentation-in-mri-brats-2015/MICCAI_BraTS2020_TrainingData\"\n",
        "patient_data_list = sorted(glob.glob(os.path.join(patient_data_folder, \"*\")))[: -2]"
      ],
      "metadata": {
        "id": "_DPbb4gMEs5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_split, test_split = compute_two_splits(patient_data_list, 0.8)"
      ],
      "metadata": {
        "id": "f9uOCf-VEs5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_volumes = compute_class_volumes(train_split)\n",
        "test_volumes = compute_class_volumes(test_split)\n",
        "visualize_class_distributions(train_volumes)"
      ],
      "metadata": {
        "id": "jmoKzmZHEs5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_class_distributions(test_volumes)"
      ],
      "metadata": {
        "id": "5XBpNObJEs5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_final, validation = compute_two_splits(train_split, 0.8)"
      ],
      "metadata": {
        "id": "2394lFbxEs5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_final_volumes = compute_class_volumes(training_final)\n",
        "validation_volumes = compute_class_volumes(validation)\n",
        "visualize_class_distributions(train_final_volumes)"
      ],
      "metadata": {
        "id": "80_ILeBsEs5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_class_distributions(validation_volumes)"
      ],
      "metadata": {
        "id": "730LT418Es5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models"
      ],
      "metadata": {
        "id": "6M_lpOFcRsJz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Copyright 2020 - 2021 MONAI Consortium\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "from typing import Tuple, Union\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from monai.networks.blocks import UnetrBasicBlock, UnetrPrUpBlock, UnetrUpBlock\n",
        "from monai.networks.blocks import UnetOutBlock\n",
        "from monai.networks.nets import ViT\n",
        "\n",
        "\n",
        "class UNETR(nn.Module):\n",
        "    \"\"\"\n",
        "    UNETR based on: \"Hatamizadeh et al.,\n",
        "    UNETR: Transformers for 3D Medical Image Segmentation <https://arxiv.org/abs/2103.10504>\"\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        out_channels: int,\n",
        "        img_size: Tuple[int, int, int],\n",
        "        feature_size: int = 16,\n",
        "        hidden_size: int = 768,\n",
        "        mlp_dim: int = 3072,\n",
        "        num_heads: int = 12,\n",
        "        pos_embed: str = \"perceptron\",\n",
        "        norm_name: Union[Tuple, str] = \"instance\",\n",
        "        conv_block: bool = False,\n",
        "        res_block: bool = True,\n",
        "        dropout_rate: float = 0.0,\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            in_channels: dimension of input channels.\n",
        "            out_channels: dimension of output channels.\n",
        "            img_size: dimension of input image.\n",
        "            feature_size: dimension of network feature size.\n",
        "            hidden_size: dimension of hidden layer.\n",
        "            mlp_dim: dimension of feedforward layer.\n",
        "            num_heads: number of attention heads.\n",
        "            pos_embed: position embedding layer type.\n",
        "            norm_name: feature normalization type and arguments.\n",
        "            conv_block: bool argument to determine if convolutional block is used.\n",
        "            res_block: bool argument to determine if residual block is used.\n",
        "            dropout_rate: faction of the input units to drop.\n",
        "\n",
        "        Examples::\n",
        "\n",
        "            # for single channel input 4-channel output with patch size of (96,96,96), feature size of 32 and batch norm\n",
        "            >>> net = UNETR(in_channels=1, out_channels=4, img_size=(96,96,96), feature_size=32, norm_name='batch')\n",
        "\n",
        "            # for 4-channel input 3-channel output with patch size of (128,128,128), conv position embedding and instance norm\n",
        "            >>> net = UNETR(in_channels=4, out_channels=3, img_size=(128,128,128), pos_embed='conv', norm_name='instance')\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        if not (0 <= dropout_rate <= 1):\n",
        "            raise AssertionError(\"dropout_rate should be between 0 and 1.\")\n",
        "\n",
        "        if hidden_size % num_heads != 0:\n",
        "            raise AssertionError(\"hidden size should be divisible by num_heads.\")\n",
        "\n",
        "        if pos_embed not in [\"conv\", \"perceptron\"]:\n",
        "            raise KeyError(f\"Position embedding layer of type {pos_embed} is not supported.\")\n",
        "\n",
        "        self.num_layers = 12\n",
        "        self.patch_size = (16, 16, 16)  #(16, 16, 16)\n",
        "        self.feat_size = (\n",
        "            img_size[0] // self.patch_size[0],\n",
        "            img_size[1] // self.patch_size[1],\n",
        "            img_size[2] // self.patch_size[2],\n",
        "        )\n",
        "        self.hidden_size = hidden_size\n",
        "        self.classification = False\n",
        "        self.vit = ViT(\n",
        "            in_channels=in_channels,\n",
        "            img_size=img_size,\n",
        "            patch_size=self.patch_size,\n",
        "            hidden_size=hidden_size,\n",
        "            mlp_dim=mlp_dim,\n",
        "            num_layers=self.num_layers,\n",
        "            num_heads=num_heads,\n",
        "            pos_embed=pos_embed,\n",
        "            classification=self.classification,\n",
        "            dropout_rate=dropout_rate,\n",
        "        )\n",
        "        self.encoder1 = UnetrBasicBlock(\n",
        "            spatial_dims=3,\n",
        "            in_channels=in_channels,\n",
        "            out_channels=feature_size,\n",
        "            kernel_size=3,\n",
        "            stride=1,\n",
        "            norm_name=norm_name,\n",
        "            res_block=res_block,\n",
        "        )\n",
        "        self.encoder2 = UnetrPrUpBlock(\n",
        "            spatial_dims=3,\n",
        "            in_channels=hidden_size,\n",
        "            out_channels=feature_size * 2,\n",
        "            num_layer=2,\n",
        "            kernel_size=3,\n",
        "            stride=1,\n",
        "            upsample_kernel_size=2,\n",
        "            norm_name=norm_name,\n",
        "            conv_block=conv_block,\n",
        "            res_block=res_block,\n",
        "        )\n",
        "        self.encoder3 = UnetrPrUpBlock(\n",
        "            spatial_dims=3,\n",
        "            in_channels=hidden_size,\n",
        "            out_channels=feature_size * 4,\n",
        "            num_layer=1,\n",
        "            kernel_size=3,\n",
        "            stride=1,\n",
        "            upsample_kernel_size=2,\n",
        "            norm_name=norm_name,\n",
        "            conv_block=conv_block,\n",
        "            res_block=res_block,\n",
        "        )\n",
        "        self.encoder4 = UnetrPrUpBlock(\n",
        "            spatial_dims=3,\n",
        "            in_channels=hidden_size,\n",
        "            out_channels=feature_size * 8,\n",
        "            num_layer=0,\n",
        "            kernel_size=3,\n",
        "            stride=1,\n",
        "            upsample_kernel_size=2,\n",
        "            norm_name=norm_name,\n",
        "            conv_block=conv_block,\n",
        "            res_block=res_block,\n",
        "        )\n",
        "        self.decoder5 = UnetrUpBlock(\n",
        "            spatial_dims=3,\n",
        "            in_channels=hidden_size,\n",
        "            out_channels=feature_size * 8,\n",
        "            kernel_size=3,\n",
        "            upsample_kernel_size=2,\n",
        "            norm_name=norm_name,\n",
        "            res_block=res_block,\n",
        "        )\n",
        "        self.decoder4 = UnetrUpBlock(\n",
        "            spatial_dims=3,\n",
        "            in_channels=feature_size * 8,\n",
        "            out_channels=feature_size * 4,\n",
        "            kernel_size=3,\n",
        "            upsample_kernel_size=2,\n",
        "            norm_name=norm_name,\n",
        "            res_block=res_block,\n",
        "        )\n",
        "        self.decoder3 = UnetrUpBlock(\n",
        "            spatial_dims=3,\n",
        "            in_channels=feature_size * 4,\n",
        "            out_channels=feature_size * 2,\n",
        "            kernel_size=3,\n",
        "            upsample_kernel_size=2,\n",
        "            norm_name=norm_name,\n",
        "            res_block=res_block,\n",
        "        )\n",
        "        self.decoder2 = UnetrUpBlock(\n",
        "            spatial_dims=3,\n",
        "            in_channels=feature_size * 2,\n",
        "            out_channels=feature_size,\n",
        "            kernel_size=3,\n",
        "            upsample_kernel_size=2,\n",
        "            norm_name=norm_name,\n",
        "            res_block=res_block,\n",
        "        )\n",
        "        self.out = UnetOutBlock(spatial_dims=3, in_channels=feature_size, out_channels=out_channels)  # type: ignore\n",
        "\n",
        "    def proj_feat(self, x, hidden_size, feat_size):\n",
        "        x = x.view(x.size(0), feat_size[0], feat_size[1], feat_size[2], hidden_size)\n",
        "        x = x.permute(0, 4, 1, 2, 3).contiguous()\n",
        "        return x\n",
        "\n",
        "    def load_from(self, weights):\n",
        "        with torch.no_grad():\n",
        "            res_weight = weights\n",
        "            # copy weights from patch embedding\n",
        "            for i in weights[\"state_dict\"]:\n",
        "                print(i)\n",
        "            self.vit.patch_embedding.position_embeddings.copy_(\n",
        "                weights[\"state_dict\"][\"module.transformer.patch_embedding.position_embeddings_3d\"]\n",
        "            )\n",
        "            self.vit.patch_embedding.cls_token.copy_(\n",
        "                weights[\"state_dict\"][\"module.transformer.patch_embedding.cls_token\"]\n",
        "            )\n",
        "            self.vit.patch_embedding.patch_embeddings[1].weight.copy_(\n",
        "                weights[\"state_dict\"][\"module.transformer.patch_embedding.patch_embeddings.1.weight\"]\n",
        "            )\n",
        "            self.vit.patch_embedding.patch_embeddings[1].bias.copy_(\n",
        "                weights[\"state_dict\"][\"module.transformer.patch_embedding.patch_embeddings.1.bias\"]\n",
        "            )\n",
        "\n",
        "            # copy weights from  encoding blocks (default: num of blocks: 12)\n",
        "            for bname, block in self.vit.blocks.named_children():\n",
        "                print(block)\n",
        "                block.loadFrom(weights, n_block=bname)\n",
        "            # last norm layer of transformer\n",
        "            self.vit.norm.weight.copy_(weights[\"state_dict\"][\"module.transformer.norm.weight\"])\n",
        "            self.vit.norm.bias.copy_(weights[\"state_dict\"][\"module.transformer.norm.bias\"])\n",
        "\n",
        "    def forward(self, x_in):\n",
        "        x, hidden_states_out = self.vit(x_in)\n",
        "        enc1 = self.encoder1(x_in)\n",
        "        x2 = hidden_states_out[3]\n",
        "        enc2 = self.encoder2(self.proj_feat(x2, self.hidden_size, self.feat_size))\n",
        "        x3 = hidden_states_out[6]\n",
        "        enc3 = self.encoder3(self.proj_feat(x3, self.hidden_size, self.feat_size))\n",
        "        x4 = hidden_states_out[9]\n",
        "        enc4 = self.encoder4(self.proj_feat(x4, self.hidden_size, self.feat_size))\n",
        "        dec4 = self.proj_feat(x, self.hidden_size, self.feat_size)\n",
        "        dec3 = self.decoder5(dec4, enc4)\n",
        "        dec2 = self.decoder4(dec3, enc3)\n",
        "        dec1 = self.decoder3(dec2, enc2)\n",
        "        out = self.decoder2(dec1, enc1)\n",
        "        logits = self.out(out)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "ooRbfGCxRttN"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Generator"
      ],
      "metadata": {
        "id": "U6uSCt9REs5B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import nibabel as nib\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "import glob\n",
        "import torchvision.transforms as transforms"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-16T23:22:16.375825Z",
          "iopub.execute_input": "2023-03-16T23:22:16.376162Z",
          "iopub.status.idle": "2023-03-16T23:22:17.814298Z",
          "shell.execute_reply.started": "2023-03-16T23:22:16.376134Z",
          "shell.execute_reply": "2023-03-16T23:22:17.813292Z"
        },
        "trusted": true,
        "id": "FXRZkhe0Es5B"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "HmR5eyycJ05w"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def resize_3d_image(original_img, size):\n",
        "    resized_img = [cv2.resize(original_img[:, :, i], (128, 128)) for i in range(original_img.shape[2])]\n",
        "    resized_img = np.moveaxis(np.stack(resized_img), 0, -1)\n",
        "    return resized_img\n",
        "\n",
        "\n",
        "class BTSDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, patient_data_list_path, src_folder, no_classes=4):\n",
        "        patient_data_list = sorted(text_file_reader(patient_data_list_path))\n",
        "        patient_data_list = [os.path.join(src_folder, i) for i in patient_data_list]\n",
        "        self.patient_flair_scans_list = [glob.glob(os.path.join(i, \"*_flair.nii.gz\"))[0] for i in patient_data_list]\n",
        "        self.patient_t1ce_scans_list = [glob.glob(os.path.join(i, \"*_t1ce.nii.gz\"))[0] for i in patient_data_list]\n",
        "        self.patient_t2_scans_list = [glob.glob(os.path.join(i, \"*_t2.nii.gz\"))[0] for i in patient_data_list]\n",
        "        self.patient_seg_scans_list = [glob.glob(os.path.join(i, \"*_seg.nii.gz\"))[0] for i in patient_data_list]\n",
        "        self.transform = transforms.ToTensor()\n",
        "        self.no_classes = no_classes\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.patient_flair_scans_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        t1ce_scan = self.transform(resize_3d_image(np.asarray(nib.load(self.patient_t1ce_scans_list[idx]).get_fdata())[:, :, 5: -6], (128 ,128)))\n",
        "        t2_scan = self.transform(resize_3d_image(np.asarray(nib.load(self.patient_t2_scans_list[idx]).get_fdata())[:, :, 5: -6], (128 ,128)))\n",
        "        flair_scan = self.transform(resize_3d_image(np.asarray(nib.load(self.patient_flair_scans_list[idx]).get_fdata())[:, :, 5: -6], (128 ,128)))\n",
        "        seg_label = resize_3d_image(np.asarray(nib.load(self.patient_seg_scans_list[idx]).get_fdata()[:, :, 5: -6]), (128 ,128))\n",
        "        seg_label[seg_label == 4] = 3\n",
        "        seg_label = self.transform(seg_label)\n",
        "        seg_label_ohe = torch.nn.functional.one_hot(seg_label.to(torch.int64), self.no_classes)\n",
        "        seg_label_ohe = torch.moveaxis(seg_label_ohe, -1, 0)\n",
        "        image_scans_stacked = torch.stack([t1ce_scan, t2_scan, flair_scan])\n",
        "        return image_scans_stacked.to(torch.float32), seg_label_ohe.to(torch.float32)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-16T23:22:17.816350Z",
          "iopub.execute_input": "2023-03-16T23:22:17.816753Z",
          "iopub.status.idle": "2023-03-16T23:22:17.826966Z",
          "shell.execute_reply.started": "2023-03-16T23:22:17.816728Z",
          "shell.execute_reply": "2023-03-16T23:22:17.825931Z"
        },
        "trusted": true,
        "id": "ur74N--UEs5B"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# path = \"/content/drive/MyDrive/Personal/MS/Brain_Tumor_segmentaion3D/dataset/BraTS2021_Training_Data\"\n",
        "# folder_list = sorted(glob.glob(os.path.join(path, \"*\")))\n",
        "# folder_list = [i for i in folder_list if os.path.isdir(i)]\n",
        "# folder_list.pop(354)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-16T23:22:17.829949Z",
          "iopub.execute_input": "2023-03-16T23:22:17.830268Z",
          "iopub.status.idle": "2023-03-16T23:22:17.887421Z",
          "shell.execute_reply.started": "2023-03-16T23:22:17.830230Z",
          "shell.execute_reply": "2023-03-16T23:22:17.886111Z"
        },
        "trusted": true,
        "id": "O-6JhRUJEs5B"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# src_folder = \"/content/drive/MyDrive/Personal/MS/Brain_Tumor_segmentaion3D\"\n",
        "# folder_list_path = \"/content/drive/MyDrive/Personal/MS/Brain_Tumor_segmentaion3D/dataset/lists/sample_100.txt\"\n",
        "# # patient_data_list = sorted(text_file_reader(folder_list_path))\n",
        "# # patient_data_list = [os.path.join(src_folder, i) for i in patient_data_list]\n",
        "# dataset = BTSDataset(folder_list_path, src_folder)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-16T23:22:19.222250Z",
          "iopub.execute_input": "2023-03-16T23:22:19.223679Z",
          "iopub.status.idle": "2023-03-16T23:22:21.085973Z",
          "shell.execute_reply.started": "2023-03-16T23:22:19.223636Z",
          "shell.execute_reply": "2023-03-16T23:22:21.085260Z"
        },
        "trusted": true,
        "id": "ihVZmy4qEs5B"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ex_idx = 0\n",
        "# ex_img, ex_label = dataset[ex_idx]\n",
        "# ex_img.shape, ex_label.shape"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-16T23:22:21.089465Z",
          "iopub.execute_input": "2023-03-16T23:22:21.090442Z",
          "iopub.status.idle": "2023-03-16T23:22:22.090807Z",
          "shell.execute_reply.started": "2023-03-16T23:22:21.090404Z",
          "shell.execute_reply": "2023-03-16T23:22:22.089945Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FDK_wANmEs5B",
        "outputId": "0f2861e1-42af-4e04-ea88-26557f3d832a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([3, 144, 128, 128]), torch.Size([4, 144, 128, 128]))"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training setup"
      ],
      "metadata": {
        "id": "gMJthQ9gEs5B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import torch\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = 'True'\n",
        "\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(self, model, loss, optimizer, number_of_epochs, weights_save_folder):\n",
        "        self.loss = instantiate_class(**loss)\n",
        "        self.number_of_epochs = number_of_epochs\n",
        "        self.model = model\n",
        "        self.model.to(\"cuda\")\n",
        "        self.optimizer = instantiate_attribute(optimizer[\"path\"])(self.model.parameters(), **optimizer[\"params\"])\n",
        "        self.best_model_weights = None\n",
        "        self.weights_save_folder = weights_save_folder\n",
        "        check_path(self.weights_save_folder)\n",
        "\n",
        "    def __call__(self, training_dataloader, validation_dataloader):\n",
        "        average_val_loss = 10 ** 6\n",
        "        for i in range(self.number_of_epochs):\n",
        "            print(\"Epoch number: \", i)\n",
        "            training_loss = []\n",
        "            for image, label in tqdm(training_dataloader):\n",
        "                self.optimizer.zero_grad()\n",
        "                probabilities = self.model(image.cuda().to(device))\n",
        "                # probabilities = self.model(image)\n",
        "                image = None\n",
        "                torch.cuda.empty_cache()\n",
        "                training_loss.append(self.loss(probabilities, label.cuda().to(device)))\n",
        "                # self.loss(probabilities, label)\n",
        "                self.optimizer.step()\n",
        "                del label, probabilities\n",
        "            print(\"Average validation loss: \", np.avg(training_loss))\n",
        "            validation_loss = []\n",
        "            print(\"Validating now\")\n",
        "            for image, label in tqdm(validation_dataloader):\n",
        "                with torch.no_grad():\n",
        "                    image = image.cuda().to(device)\n",
        "                    label = label.cuda().to(device)\n",
        "                    probabilities = self.model(image)\n",
        "                    image = None\n",
        "                    torch.cuda.empty_cache()\n",
        "                    validation_loss.append(self.loss(probabilities, label).cpu())\n",
        "                    del probabilities, label\n",
        "            print(\"Average validation loss: \", np.avg(validation_loss))\n",
        "            if np.average(validation_loss) < average_val_loss:\n",
        "                self.best_model_weights = self.model.state_dict()\n",
        "                average_val_loss = np.average(validation_loss)\n",
        "                torch.save(self.model.state_dict(), os.path.join(self.weights_save_folder, 'model_weights_%d.pth' % i))\n",
        "        return self.best_model_weights, self.model.to(\"cpu\")\n"
      ],
      "metadata": {
        "id": "vDb5ERXDEs5B"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "Pr3GqZQWEs5C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "\n",
        "def generate_predictions(model, test_dataloader):\n",
        "    model.to(device)\n",
        "    test_predictions = []\n",
        "    test_labels = []\n",
        "    with torch.no_grad():\n",
        "        for image, label in test_dataloader:\n",
        "            image = image.cuda().to(device)\n",
        "            label = label.cpu().numpy()\n",
        "            label = np.argmax(label, -1)\n",
        "            probabilities = model(image)\n",
        "            test_predictions.extend(np.argmax(probabilities.cpu(), -1))\n",
        "            test_labels.extend(label)\n",
        "            del probabilities, label\n",
        "    return test_predictions, test_labels\n"
      ],
      "metadata": {
        "id": "UTUFqx8aEs5C"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiments"
      ],
      "metadata": {
        "id": "te44s1acQy1x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists(\"/content/Experiemnts\"):\n",
        "  os.mkdir(\"/content/Experiemnts\")\n",
        "if not os.path.exists(\"/content/Experiemnts/test\"):\n",
        "  os.mkdir(\"/content/Experiemnts/test\")"
      ],
      "metadata": {
        "id": "sIzjwmnbSlxO"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config_path = \"/content/drive/MyDrive/Personal/MS/Brain_Tumor_segmentaion3D/source_code/configs/overfitting_test.yaml\"\n",
        "with open(config_path, \"r\") as f:\n",
        "    config = yaml.safe_load(f)"
      ],
      "metadata": {
        "id": "gJ4BPuBeQz9O"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = UNETR(**config[\"Model\"])"
      ],
      "metadata": {
        "id": "v47LkWRnSPiJ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_datagenerator = BTSDataset(**config[\"Training_Dataset\"])\n",
        "training_dataloader = torch.utils.data.DataLoader(training_datagenerator, batch_size=1,\n",
        "                                                  shuffle=True)"
      ],
      "metadata": {
        "id": "BSa5VpZ9SRrU"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(**config[\"Trainer\"], model=model, weights_save_folder=config[\"save_folder\"])"
      ],
      "metadata": {
        "id": "uRpuEXZMSXRK"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_weights, trained_model = trainer(training_dataloader, training_dataloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "45q-u95OXRtq",
        "outputId": "16489ca9-c4ee-4daa-bc30-b87382da8921"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch number:  0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  4%|▍         | 4/100 [00:33<13:17,  8.31s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-64accb104979>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrained_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-ff1e64117645>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, training_dataloader, validation_dataloader)\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m                 \u001b[0mprobabilities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m                 \u001b[0;31m# probabilities = self.model(image)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                 \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-1c7960c583a4>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x_in)\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0mdec3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0mdec2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m         \u001b[0mdec1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/monai/networks/blocks/unetr_block.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inp, skip)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;31m# number of channels for skip should equals to out_channels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransp_conv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, output_size)\u001b[0m\n\u001b[1;32m   1106\u001b[0m             num_spatial_dims, self.dilation)  # type: ignore[arg-type]\n\u001b[1;32m   1107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1108\u001b[0;31m         return F.conv_transpose3d(\n\u001b[0m\u001b[1;32m   1109\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1110\u001b[0m             output_padding, self.groups, self.dilation)\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 0; 14.75 GiB total capacity; 13.60 GiB already allocated; 4.81 MiB free; 13.68 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BT62Xjp6ePsy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}